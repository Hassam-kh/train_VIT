import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
import timm  # Import the timm library for ViT
from sklearn.model_selection import train_test_split

# Check if a GPU is available, and if so, use it
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Define image preprocessing and data augmentation
data_transforms = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

data_dir = '/kaggle/input/video-frames/img'
all_data = ImageFolder(data_dir, data_transforms)

# Split the dataset into training and validation
trash, use = train_test_split(all_data, test_size=0.2, random_state=42)
train_data, val_data = train_test_split(use, test_size=0.2, random_state=42)

dataloaders = {
    'train': DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4),
    'val': DataLoader(val_data, batch_size=32, shuffle=False, num_workers=4)
}

# Load the pre-trained ViT model using timm
model = timm.create_model('vit_base_patch16_224', pretrained=True)

# Modify the model for your classification task (e.g., change the output layer)
# You should replace this with the number of classes in your dataset
num_classes = len(all_data.classes)
model.head = nn.Linear(model.head.in_features, num_classes)

# Move the model to the GPU
model = model.to(device)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-5)

# Train the model
num_epochs = 1  # You can adjust the number of epochs
for epoch in range(num_epochs):
    for phase in ['train', 'val']:
        if phase == 'train':
            model.train()
        else:
            model.eval()

        running_loss = 0.0
        corrects = 0

        for inputs, labels in dataloaders[phase]:
            # Move data to the GPU
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()

            with torch.set_grad_enabled(phase == 'train'):
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)

                if phase == 'train':
                    loss.backward()
                    optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(all_data)
        epoch_acc = corrects.double() / len(all_data)

        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

# Inference with a single image
import cv2
from PIL import Image
from torchvision.transforms import ToTensor

# Load and preprocess the input image
image_path = '/kaggle/input/video-frames/img'
image = Image.open(image_path).convert('RGB')
image = data_transforms(image)
image = image.unsqueeze(0)  # Add batch dimension

# Move the input image to the GPU
image = image.to(device)

# Make a prediction
model.eval()
with torch.no_grad():
    outputs = model(image)

# Get the class with the highest probability
_, predicted_class = torch.max(outputs, 1)
print(predicted_class.item())
